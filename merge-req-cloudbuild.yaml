steps:
# Cloudbuild only clone the current commit, it won't clone the history. 
# So added below 3 steps to clone the complete history from the Github.

# Access the id_github file from Secret Manager, and setup SSH to clone the Github repo.
- name: 'gcr.io/cloud-builders/git'
  secretEnv: ['SSH_KEY']
  entrypoint: 'bash'
  args:
  - -c
  - |
    echo 'Build:$BUILD_ID triggered after the merge to branch:${BRANCH_NAME}.'
    echo 'The commit is:$COMMIT_SHA'
    echo "$$SSH_KEY" >> /root/.ssh/id_rsa
    chmod 400 /root/.ssh/id_rsa
    cp known_hosts.github /root/.ssh/known_hosts
  volumes:
  - name: 'ssh'
    path: /root/.ssh

# This just gets current branch history of the repository. But this will not copy/clone other branches.
- name: 'gcr.io/cloud-builders/git'
  args: 
  - fetch
  - --unshallow
  - git@github.com:w-pay/$REPO_NAME
  volumes:
  - name: 'ssh'
    path: /root/.ssh

# list the files which are changed in the current commit/pull request.
- name: gcr.io/cloud-builders/gcloud
  entrypoint: /bin/sh
  args: 
    - '-c'
    - | 
      ls -ltra && \
      git diff --name-only HEAD~1 HEAD > /workspace/commitfiles.txt && \
      cat /workspace/commitfiles.txt



# Synch the Dags with the GCS bucket. It creates the directories if it doesn't exists.
# In gsutil -c option is used to do the checksum comparison rather than mtime check, 
# as mtime always gets changed when repo gets cloned to cloudbuild workspace. 
# -n option would check the difference without performing the sync
# The -d option will delete files from the destination folder if they don't exist in the source folder.
# -x to exclude files. Example: -x "zlibpdh/__init__.py|airflow_monitoring.py$"
- name: gcr.io/cloud-builders/gsutil
  entrypoint: bash
  args: 
  - '-c'
  - | 
    # if grep app\/dags\/ /workspace/commitfiles.txt;
    #  then
      echo "*********** Dags sync to bucket: ${_GCS_BUCKET} started ************"
    # gsutil -m rsync -c -r -x "zlibpdh/pdh_utilities.py|zlibpdh/composer_constants.py|airflow_monitoring.py$" app/dags/common gs://${_GCS_BUCKET}/dags;
      gsutil -m rsync -c -r -x "pdh_billing_engine_etl.py|pdh_file_gen_etl.py|zlibpdh/composer_constants.py|airflow_monitoring.py$" app/dags/common gs://${_GCS_BUCKET}/dags;
      echo "*********** Dags sync end ************"
      if [[ $BRANCH_NAME == "preprod" ]];
        then 
          echo "********* Copying the preprod only dags files*******"
    #     gsutil -m rsync -c -r -n -x "zlibpdh/pdh_utilities.py$" app/dags/preprod gs://${_GCS_BUCKET}/dags;
          gsutil -m rsync -c -r -n -x "pdh_billing_engine_etl.py|pdh_file_gen_etl.py$" app/dags/preprod gs://${_GCS_BUCKET}/dags;
          echo "********* End of preprod copy files*******"
      fi
    #  else
    #   echo "Dags are not modified, hence dags sync with gcs: ${_GCS_BUCKET} is not required.";
    # fi

# Loading ETL files to GCS bucket. 
- name: gcr.io/cloud-builders/gsutil
  entrypoint: bash
  args: 
  - '-c'
  - | 
      # if grep app\/etl\/ /workspace/commitfiles.txt;
      # then
        echo "***********ETL Files upload to bucket: ${_ETL_BUCKET} started************"
        if [[ $BRANCH_NAME == "preprod" ]];
          then 
            gsutil -m rsync -c -r  app/etl gs://${_ETL_BUCKET};     
          else
            gsutil -m rsync -c -r -x "prod_replicator\/.*$" app/etl gs://${_ETL_BUCKET};
        fi
        echo "*********** ETL Files upload ended ************"
      # else
      #   echo "etl files are not modified/added, hence etl sync with gcs: ${_ETL_BUCKET} is not required.";
      # fi
# logsBucket: "gs://${_LOGS_BUCKET}"
# cloudbuild-github-ssh-key

availableSecrets:
  secretManager:
  # - versionName: projects/$PROJECT_NUMBER/secrets/$REPO_NAME-github-key/versions/latest
  - versionName: projects/$PROJECT_NUMBER/secrets/cloudbuild-github-ssh-key/versions/latest
    env: 'SSH_KEY'


# # Terraform apply after the merge.
# - name: hashicorp/terraform:1.1.2
#   entrypoint: sh
#   args: 
#   - '-c'
#   - |
#     cd app/terraform/
#     terraform init -backend-config "bucket=${_TF_BACKEND_BUCKET}" -backend-config "prefix=${_TF_BACKEND_PREFIX}"
#     # terraform workspace select ${_BASE_BRANCH} || terraform workspace new ${_BASE_BRANCH}
#     terraform plan
#     # terraform apply --auto-approve
#     if grep "bq\/tables\/\|bq\/views\/\|bq/etl\/" /workspace/commitfiles.txt; 
#     then 
#       echo "bq deployment is required"; 
#       if grep bq\/tables\/ /workspace/commitfiles.txt;
#       then
#         echo "bq table deployment is required. Also check if table backup is required."; 

#       else
#         echo "bq table deployment is not required in current build.";
#       fi
#       if grep bq\/views\/ /workspace/commitfiles.txt;
#       then
#         echo "bq views deployment is required."; 
#       else
#         echo "bq views deployment is not required in current build.";
#       fi
#       if grep bq\/etl\/ /workspace/commitfiles.txt;
#       then
#         echo "bq etl deployment is required. Also check if etl execution is required."; 
#       else
#         echo "bq etl deployment is not required in current build.";
#       fi   
#     else 
#       echo "No bq files modified in current commit, hence bq deployment is not required."; 
#     fi   
